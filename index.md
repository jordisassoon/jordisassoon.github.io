---
layout: default
title: Home
---

I'm a Performance Engineer at Aleph Alpha working on high-performance infrastructure for large language models.

My work focuses on optimizing training and inference of transformer-based systems through massively parallel computation, custom CUDA kernels, and efficient parallelization strategies.

## Current Work

At Aleph Alpha, I develop and optimize performance-critical components of large-scale language model systems, including:

- CUDA kernel development and GPU performance optimization  
- Parallelization strategies for large-scale model training and inference  
- High-throughput and low-latency LLM inference systems  
- Performance engineering for production AI workloads  
- Benchmarking and evaluation across diverse model architectures  

My goal is to make large language models faster, more efficient, and more scalable.

## Background

Previously, I worked on efficient deep learning and computer vision systems, including publishing research on integer-only Vision Transformers and developing highly optimized training pipelines achieving significant improvements in speed, memory efficiency, and scalability.

## Links

- [LinkedIn](https://www.linkedin.com/in/jordan-sassoon/)
- [GitHub](https://github.com/jordisassoon)
- [Google Scholar](https://scholar.google.com/citations?user=o0_U5WcAAAAJ)

## Contact

jordan.sassoon@aleph-alpha-research.com